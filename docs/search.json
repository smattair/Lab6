[
  {
    "objectID": "hyperparameter-tuning.html",
    "href": "hyperparameter-tuning.html",
    "title": "Load Packages",
    "section": "",
    "text": "Load Packages\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.1.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(readr)\nlibrary(powerjoin)\nlibrary(skimr)\nlibrary(visdat)\nlibrary(ggpubr)\nlibrary(ggcorrplot)\nlibrary(workflowsets)\nlibrary(recipes)\n\n\n# List all .txt files in the data folder\ndata_files &lt;- list.files(\"data\", pattern = \"\\\\.txt$\", full.names = TRUE)\n\n# Read each file using read_delim and store in a list\ncamels_list &lt;- map(data_files, read_delim, delim = \";\")\n\nRows: 671 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr (3): gauge_id, high_prec_timing, low_prec_timing\ndbl (9): p_mean, pet_mean, p_seasonality, frac_snow, aridity, high_prec_freq...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 671 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr (3): gauge_id, geol_1st_class, geol_2nd_class\ndbl (5): glim_1st_class_frac, glim_2nd_class_frac, carbonate_rocks_frac, geo...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 671 Columns: 14\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr  (1): gauge_id\ndbl (13): q_mean, runoff_ratio, slope_fdc, baseflow_index, stream_elas, q5, ...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 671 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr  (1): gauge_id\ndbl (11): soil_depth_pelletier, soil_depth_statsgo, soil_porosity, soil_cond...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 671 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr (1): gauge_id\ndbl (6): gauge_lat, gauge_lon, elev_mean, slope_mean, area_gages2, area_geos...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nRows: 671 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \";\"\nchr (2): gauge_id, dom_land_cover\ndbl (8): frac_forest, lai_max, lai_diff, gvf_max, gvf_diff, dom_land_cover_f...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Join all data frames using power_full_join by gauge_id\ncamels_data &lt;- reduce(camels_list, power_full_join, by = \"gauge_id\")\n\n\n# Look at structure of the combined data\nglimpse(camels_data)\n\nRows: 671\nColumns: 58\n$ gauge_id             &lt;chr&gt; \"01013500\", \"01022500\", \"01030500\", \"01031500\", \"…\n$ p_mean               &lt;dbl&gt; 3.126679, 3.608126, 3.274405, 3.522957, 3.323146,…\n$ pet_mean             &lt;dbl&gt; 1.971555, 2.119256, 2.043594, 2.071324, 2.090024,…\n$ p_seasonality        &lt;dbl&gt; 0.187940259, -0.114529586, 0.047358189, 0.1040905…\n$ frac_snow            &lt;dbl&gt; 0.3134404, 0.2452590, 0.2770184, 0.2918365, 0.280…\n$ aridity              &lt;dbl&gt; 0.6305587, 0.5873564, 0.6241114, 0.5879503, 0.628…\n$ high_prec_freq       &lt;dbl&gt; 12.95, 20.55, 17.15, 18.90, 20.10, 13.50, 17.50, …\n$ high_prec_dur        &lt;dbl&gt; 1.348958, 1.205279, 1.207746, 1.148936, 1.165217,…\n$ high_prec_timing     &lt;chr&gt; \"son\", \"son\", \"son\", \"son\", \"son\", \"jja\", \"son\", …\n$ low_prec_freq        &lt;dbl&gt; 202.20, 233.65, 215.60, 227.35, 235.90, 193.50, 2…\n$ low_prec_dur         &lt;dbl&gt; 3.427119, 3.662226, 3.514262, 3.473644, 3.691706,…\n$ low_prec_timing      &lt;chr&gt; \"mam\", \"jja\", \"djf\", \"djf\", \"djf\", \"mam\", \"mam\", …\n$ geol_1st_class       &lt;chr&gt; \"Siliciclastic sedimentary rocks\", \"Acid plutonic…\n$ glim_1st_class_frac  &lt;dbl&gt; 0.8159044, 0.5906582, 0.5733054, 0.4489279, 0.308…\n$ geol_2nd_class       &lt;chr&gt; \"Basic volcanic rocks\", \"Siliciclastic sedimentar…\n$ glim_2nd_class_frac  &lt;dbl&gt; 0.17972945, 0.16461821, 0.28701001, 0.44386282, 0…\n$ carbonate_rocks_frac &lt;dbl&gt; 0.000000000, 0.000000000, 0.052140094, 0.02625797…\n$ geol_porostiy        &lt;dbl&gt; 0.1714, 0.0710, 0.1178, 0.0747, 0.0522, 0.0711, 0…\n$ geol_permeability    &lt;dbl&gt; -14.7019, -14.2138, -14.4918, -14.8410, -14.4819,…\n$ q_mean               &lt;dbl&gt; 1.699155, 2.173062, 1.820108, 2.030242, 2.182870,…\n$ runoff_ratio         &lt;dbl&gt; 0.5434375, 0.6022689, 0.5558590, 0.5762893, 0.656…\n$ slope_fdc            &lt;dbl&gt; 1.528219, 1.776280, 1.871110, 1.494019, 1.415939,…\n$ baseflow_index       &lt;dbl&gt; 0.5852260, 0.5544784, 0.5084407, 0.4450905, 0.473…\n$ stream_elas          &lt;dbl&gt; 1.8453242, 1.7027824, 1.3775052, 1.6486930, 1.510…\n$ q5                   &lt;dbl&gt; 0.24110613, 0.20473436, 0.10714920, 0.11134535, 0…\n$ q95                  &lt;dbl&gt; 6.373021, 7.123049, 6.854887, 8.010503, 8.095148,…\n$ high_q_freq          &lt;dbl&gt; 6.10, 3.90, 12.25, 18.90, 14.95, 14.10, 16.05, 16…\n$ high_q_dur           &lt;dbl&gt; 8.714286, 2.294118, 7.205882, 3.286957, 2.577586,…\n$ low_q_freq           &lt;dbl&gt; 41.35, 65.15, 89.25, 94.80, 71.55, 58.90, 82.20, …\n$ low_q_dur            &lt;dbl&gt; 20.170732, 17.144737, 19.402174, 14.697674, 12.77…\n$ zero_q_freq          &lt;dbl&gt; 0.00000000, 0.00000000, 0.00000000, 0.00000000, 0…\n$ hfd_mean             &lt;dbl&gt; 207.25, 166.25, 184.90, 181.00, 184.80, 197.20, 1…\n$ soil_depth_pelletier &lt;dbl&gt; 7.4047619, 17.4128079, 19.0114144, 7.2525570, 5.3…\n$ soil_depth_statsgo   &lt;dbl&gt; 1.248408, 1.491846, 1.461363, 1.279047, 1.392779,…\n$ soil_porosity        &lt;dbl&gt; 0.4611488, 0.4159055, 0.4590910, 0.4502360, 0.422…\n$ soil_conductivity    &lt;dbl&gt; 1.106522, 2.375005, 1.289807, 1.373292, 2.615154,…\n$ max_water_content    &lt;dbl&gt; 0.5580548, 0.6262289, 0.6530198, 0.5591227, 0.561…\n$ sand_frac            &lt;dbl&gt; 27.84183, 59.39016, 32.23546, 35.26903, 55.16313,…\n$ silt_frac            &lt;dbl&gt; 55.15694, 28.08094, 51.77918, 50.84123, 34.18544,…\n$ clay_frac            &lt;dbl&gt; 16.275732, 12.037646, 14.776824, 12.654125, 10.30…\n$ water_frac           &lt;dbl&gt; 5.3766978, 1.2269127, 1.6343449, 0.6745936, 0.000…\n$ organic_frac         &lt;dbl&gt; 0.4087168, 0.0000000, 1.3302776, 0.0000000, 0.000…\n$ other_frac           &lt;dbl&gt; 0.0000000, 0.3584723, 0.0220161, 0.0000000, 0.147…\n$ gauge_lat            &lt;dbl&gt; 47.23739, 44.60797, 45.50097, 45.17501, 44.86920,…\n$ gauge_lon            &lt;dbl&gt; -68.58264, -67.93524, -68.30596, -69.31470, -69.9…\n$ elev_mean            &lt;dbl&gt; 250.31, 92.68, 143.80, 247.80, 310.38, 615.70, 47…\n$ slope_mean           &lt;dbl&gt; 21.64152, 17.79072, 12.79195, 29.56035, 49.92122,…\n$ area_gages2          &lt;dbl&gt; 2252.70, 573.60, 3676.17, 769.05, 909.10, 383.82,…\n$ area_geospa_fabric   &lt;dbl&gt; 2303.95, 620.38, 3676.09, 766.53, 904.94, 396.10,…\n$ frac_forest          &lt;dbl&gt; 0.9063, 0.9232, 0.8782, 0.9548, 0.9906, 1.0000, 1…\n$ lai_max              &lt;dbl&gt; 4.167304, 4.871392, 4.685200, 4.903259, 5.086811,…\n$ lai_diff             &lt;dbl&gt; 3.340732, 3.746692, 3.665543, 3.990843, 4.300978,…\n$ gvf_max              &lt;dbl&gt; 0.8045674, 0.8639358, 0.8585020, 0.8706685, 0.891…\n$ gvf_diff             &lt;dbl&gt; 0.3716482, 0.3377125, 0.3513934, 0.3986194, 0.445…\n$ dom_land_cover_frac  &lt;dbl&gt; 0.8834519, 0.8204934, 0.9752580, 1.0000000, 0.850…\n$ dom_land_cover       &lt;chr&gt; \"    Mixed Forests\", \"    Mixed Forests\", \"    Mi…\n$ root_depth_50        &lt;dbl&gt; NA, 0.2374345, NA, 0.2500000, 0.2410270, 0.225615…\n$ root_depth_99        &lt;dbl&gt; NA, 2.238444, NA, 2.400000, 2.340180, 2.237435, 2…\n\n# Summarize the dataset\nskim(camels_data)\n\n\nData summary\n\n\nName\ncamels_data\n\n\nNumber of rows\n671\n\n\nNumber of columns\n58\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nnumeric\n52\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ngauge_id\n0\n1.00\n8\n8\n0\n671\n0\n\n\nhigh_prec_timing\n0\n1.00\n3\n3\n0\n4\n0\n\n\nlow_prec_timing\n0\n1.00\n3\n3\n0\n4\n0\n\n\ngeol_1st_class\n0\n1.00\n12\n31\n0\n12\n0\n\n\ngeol_2nd_class\n138\n0.79\n12\n31\n0\n13\n0\n\n\ndom_land_cover\n0\n1.00\n12\n38\n0\n12\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\np_mean\n0\n1.00\n3.26\n1.41\n0.64\n2.37\n3.23\n3.78\n8.94\n▃▇▂▁▁\n\n\npet_mean\n0\n1.00\n2.79\n0.55\n1.90\n2.34\n2.69\n3.15\n4.74\n▇▇▅▂▁\n\n\np_seasonality\n0\n1.00\n-0.04\n0.53\n-1.44\n-0.26\n0.08\n0.22\n0.92\n▁▂▃▇▂\n\n\nfrac_snow\n0\n1.00\n0.18\n0.20\n0.00\n0.04\n0.10\n0.22\n0.91\n▇▂▁▁▁\n\n\naridity\n0\n1.00\n1.06\n0.62\n0.22\n0.70\n0.86\n1.27\n5.21\n▇▂▁▁▁\n\n\nhigh_prec_freq\n0\n1.00\n20.93\n4.55\n7.90\n18.50\n22.00\n24.23\n32.70\n▂▃▇▇▁\n\n\nhigh_prec_dur\n0\n1.00\n1.35\n0.19\n1.08\n1.21\n1.28\n1.44\n2.09\n▇▅▂▁▁\n\n\nlow_prec_freq\n0\n1.00\n254.65\n35.12\n169.90\n232.70\n255.85\n278.92\n348.70\n▂▅▇▅▁\n\n\nlow_prec_dur\n0\n1.00\n5.95\n3.20\n2.79\n4.24\n4.95\n6.70\n36.51\n▇▁▁▁▁\n\n\nglim_1st_class_frac\n0\n1.00\n0.79\n0.20\n0.30\n0.61\n0.83\n1.00\n1.00\n▁▃▃▃▇\n\n\nglim_2nd_class_frac\n0\n1.00\n0.16\n0.14\n0.00\n0.00\n0.14\n0.27\n0.49\n▇▃▃▂▁\n\n\ncarbonate_rocks_frac\n0\n1.00\n0.12\n0.26\n0.00\n0.00\n0.00\n0.04\n1.00\n▇▁▁▁▁\n\n\ngeol_porostiy\n3\n1.00\n0.13\n0.07\n0.01\n0.07\n0.13\n0.19\n0.28\n▇▆▇▇▂\n\n\ngeol_permeability\n0\n1.00\n-13.89\n1.18\n-16.50\n-14.77\n-13.96\n-13.00\n-10.90\n▂▅▇▅▂\n\n\nq_mean\n1\n1.00\n1.49\n1.54\n0.00\n0.63\n1.13\n1.75\n9.69\n▇▁▁▁▁\n\n\nrunoff_ratio\n1\n1.00\n0.39\n0.23\n0.00\n0.24\n0.35\n0.51\n1.36\n▆▇▂▁▁\n\n\nslope_fdc\n1\n1.00\n1.24\n0.51\n0.00\n0.90\n1.28\n1.63\n2.50\n▂▅▇▇▁\n\n\nbaseflow_index\n0\n1.00\n0.49\n0.16\n0.01\n0.40\n0.50\n0.60\n0.98\n▁▃▇▅▁\n\n\nstream_elas\n1\n1.00\n1.83\n0.78\n-0.64\n1.32\n1.70\n2.23\n6.24\n▁▇▃▁▁\n\n\nq5\n1\n1.00\n0.17\n0.27\n0.00\n0.01\n0.08\n0.22\n2.42\n▇▁▁▁▁\n\n\nq95\n1\n1.00\n5.06\n4.94\n0.00\n2.07\n3.77\n6.29\n31.82\n▇▂▁▁▁\n\n\nhigh_q_freq\n1\n1.00\n25.74\n29.07\n0.00\n6.41\n15.10\n35.79\n172.80\n▇▂▁▁▁\n\n\nhigh_q_dur\n1\n1.00\n6.91\n10.07\n0.00\n1.82\n2.85\n7.55\n92.56\n▇▁▁▁▁\n\n\nlow_q_freq\n1\n1.00\n107.62\n82.24\n0.00\n37.44\n96.00\n162.14\n356.80\n▇▆▅▂▁\n\n\nlow_q_dur\n1\n1.00\n22.28\n21.66\n0.00\n10.00\n15.52\n26.91\n209.88\n▇▁▁▁▁\n\n\nzero_q_freq\n1\n1.00\n0.03\n0.11\n0.00\n0.00\n0.00\n0.00\n0.97\n▇▁▁▁▁\n\n\nhfd_mean\n1\n1.00\n182.52\n33.53\n112.25\n160.16\n173.77\n204.05\n287.75\n▂▇▃▂▁\n\n\nsoil_depth_pelletier\n0\n1.00\n10.87\n16.24\n0.27\n1.00\n1.23\n12.89\n50.00\n▇▁▁▁▁\n\n\nsoil_depth_statsgo\n0\n1.00\n1.29\n0.27\n0.40\n1.11\n1.46\n1.50\n1.50\n▁▁▂▂▇\n\n\nsoil_porosity\n0\n1.00\n0.44\n0.02\n0.37\n0.43\n0.44\n0.46\n0.68\n▃▇▁▁▁\n\n\nsoil_conductivity\n0\n1.00\n1.74\n1.52\n0.45\n0.93\n1.35\n1.93\n13.96\n▇▁▁▁▁\n\n\nmax_water_content\n0\n1.00\n0.53\n0.15\n0.09\n0.43\n0.56\n0.64\n1.05\n▁▅▇▃▁\n\n\nsand_frac\n0\n1.00\n36.47\n15.63\n8.18\n25.44\n35.27\n44.46\n91.98\n▅▇▅▁▁\n\n\nsilt_frac\n0\n1.00\n33.86\n13.25\n2.99\n23.95\n34.06\n43.64\n67.77\n▂▆▇▆▁\n\n\nclay_frac\n0\n1.00\n19.89\n9.32\n1.85\n14.00\n18.66\n25.42\n50.35\n▃▇▅▂▁\n\n\nwater_frac\n0\n1.00\n0.10\n0.94\n0.00\n0.00\n0.00\n0.00\n19.35\n▇▁▁▁▁\n\n\norganic_frac\n0\n1.00\n0.59\n3.84\n0.00\n0.00\n0.00\n0.00\n57.86\n▇▁▁▁▁\n\n\nother_frac\n0\n1.00\n9.82\n16.83\n0.00\n0.00\n1.31\n11.74\n99.38\n▇▁▁▁▁\n\n\ngauge_lat\n0\n1.00\n39.24\n5.21\n27.05\n35.70\n39.25\n43.21\n48.82\n▂▃▇▆▅\n\n\ngauge_lon\n0\n1.00\n-95.79\n16.21\n-124.39\n-110.41\n-92.78\n-81.77\n-67.94\n▆▃▇▇▅\n\n\nelev_mean\n0\n1.00\n759.42\n786.00\n10.21\n249.67\n462.72\n928.88\n3571.18\n▇▂▁▁▁\n\n\nslope_mean\n0\n1.00\n46.20\n47.12\n0.82\n7.43\n28.80\n73.17\n255.69\n▇▂▂▁▁\n\n\narea_gages2\n0\n1.00\n792.62\n1701.95\n4.03\n122.28\n329.68\n794.30\n25791.04\n▇▁▁▁▁\n\n\narea_geospa_fabric\n0\n1.00\n808.08\n1709.85\n4.10\n127.98\n340.70\n804.50\n25817.78\n▇▁▁▁▁\n\n\nfrac_forest\n0\n1.00\n0.64\n0.37\n0.00\n0.28\n0.81\n0.97\n1.00\n▃▁▁▂▇\n\n\nlai_max\n0\n1.00\n3.22\n1.52\n0.37\n1.81\n3.37\n4.70\n5.58\n▅▆▃▅▇\n\n\nlai_diff\n0\n1.00\n2.45\n1.33\n0.15\n1.20\n2.34\n3.76\n4.83\n▇▇▇▆▇\n\n\ngvf_max\n0\n1.00\n0.72\n0.17\n0.18\n0.61\n0.78\n0.86\n0.92\n▁▁▂▃▇\n\n\ngvf_diff\n0\n1.00\n0.32\n0.15\n0.03\n0.19\n0.32\n0.46\n0.65\n▃▇▅▇▁\n\n\ndom_land_cover_frac\n0\n1.00\n0.81\n0.18\n0.31\n0.65\n0.86\n1.00\n1.00\n▁▂▃▃▇\n\n\nroot_depth_50\n24\n0.96\n0.18\n0.03\n0.12\n0.17\n0.18\n0.19\n0.25\n▃▃▇▂▂\n\n\nroot_depth_99\n24\n0.96\n1.83\n0.30\n1.50\n1.52\n1.80\n2.00\n3.10\n▇▃▂▁▁\n\n\n\n\n# Visualize missing data\nvis_miss(camels_data)\n\n\n\n\n\n\n\n# Drop variables with more than 20% missing data\ncamels_clean &lt;- camels_data %&gt;%\n  select(where(~ mean(!is.na(.)) &gt; 0.8)) %&gt;%\n  drop_na(q_mean)  # Ensure response variable is present\n\n# Look at correlation of numeric variables\nnumeric_vars &lt;- camels_clean %&gt;% select(where(is.numeric))\n\n# Plot correlation matrix\ncor_matrix &lt;- cor(numeric_vars, use = \"complete.obs\")\nggcorrplot(cor_matrix, lab = TRUE, lab_size = 2.5)\n\n\n\n\n\n\n\n\n\n# Set a seed for reproducibility\nset.seed(123)\n\n# Split the cleaned dataset: 80% training, 20% testing\ncamels_split &lt;- initial_split(camels_clean, prop = 0.8)\n\n# Extract training and testing sets\ncamels_train &lt;- training(camels_split)\ncamels_test &lt;- testing(camels_split)\n\n# Optional: check dimensions\ndim(camels_train)\n\n[1] 536  57\n\ndim(camels_test)\n\n[1] 134  57\n\n\n\nlibrary(recipes)\n\nrecipe_data &lt;- recipe(~ ., data = camels_data) %&gt;%\n  step_zv(all_predictors()) %&gt;%  # Remove zero variance predictors\n  step_novel(all_nominal()) %&gt;%  # Handle new levels in categorical variables\n  step_unknown(all_nominal()) %&gt;%  # Handle unknown levels in categorical variables\n  step_corr(all_predictors(), threshold = 0.9) %&gt;%  # Remove highly correlated predictors\n  step_impute_median(all_numeric()) %&gt;%  # Impute missing numeric values using median\n  step_impute_mode(all_nominal()) %&gt;%  # Impute missing nominal values using mode\n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%  # Create dummy variables for nominal features\n  step_normalize(all_numeric())  # Normalize numeric predictors\n\n\nlibrary(tidymodels)\n\n# Clean the training data by removing rows with missing or infinite q_mean\ncamels_train_cleaned &lt;- camels_train %&gt;%\n  filter(!is.na(q_mean) & !is.infinite(q_mean) & q_mean != -Inf & q_mean != Inf) %&gt;%\n  drop_na()  # Drop any other rows with missing values\n\nlibrary(recipes)\n\ncamels_recipe &lt;- recipe(runoff_ratio ~ ., data = camels_train) %&gt;%\n  step_zv() %&gt;%  # Remove predictors with zero variance\n  step_novel(all_nominal_predictors()) %&gt;%\n  step_unknown(all_nominal_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors())\n\n# Create 10-fold cross-validation folds\ncamels_folds &lt;- vfold_cv(camels_train_cleaned, v = 10)\n\n# Define model specifications without tuning\nlin_reg_spec &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\n# Random Forest with fixed parameters\nrf_spec &lt;- rand_forest(mtry = 3, trees = 500) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\n# XGBoost with fixed parameters\nxgb_spec &lt;- boost_tree(mtry = 3, trees = 500, learn_rate = 0.1) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n# Create the workflow set\nworkflow_set &lt;- workflow_set(\n  preproc = list(camels = camels_recipe),\n  models = list(\n    lin_reg = lin_reg_spec,\n    random_forest = rf_spec,\n    xgboost = xgb_spec\n  )\n)\n\n# Fit the workflows with cross-validation and evaluate using RMSE and R-squared\nresults &lt;- workflow_map(\n  workflow_set,\n  resamples = camels_folds,\n  metrics = metric_set(rmse, rsq),\n  verbose = TRUE\n)\n\ni   No tuning parameters. `fit_resamples()` will be attempted\n\n\ni 1 of 3 resampling: camels_lin_reg\n\n\n→ A | warning: !  The following columns have zero variance so scaling cannot be used:\n                 gauge_id_new, gauge_id_unknown, high_prec_timing_new,\n                 high_prec_timing_unknown, low_prec_timing_new, low_prec_timing_unknown,\n                 geol_1st_class_new, geol_1st_class_unknown, dom_land_cover_new, and\n                 dom_land_cover_unknown.\n               ℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n                 before normalizing.\n\n\nThere were issues with some computations   A: x1\n\n\n→ B | warning: prediction from rank-deficient fit; consider predict(., rankdeficient=\"NA\")\n\n\nThere were issues with some computations   A: x1\nThere were issues with some computations   A: x2   B: x1\nThere were issues with some computations   A: x10   B: x10\n\n✔ 1 of 3 resampling: camels_lin_reg (2.8s)\ni   No tuning parameters. `fit_resamples()` will be attempted\ni 2 of 3 resampling: camels_random_forest\n→ A | warning: !  The following columns have zero variance so scaling cannot be used:\n                 gauge_id_new, gauge_id_unknown, high_prec_timing_new,\n                 high_prec_timing_unknown, low_prec_timing_new, low_prec_timing_unknown,\n                 geol_1st_class_new, geol_1st_class_unknown, dom_land_cover_new, and\n                 dom_land_cover_unknown.\n               ℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n                 before normalizing.\nThere were issues with some computations   A: x3\nThere were issues with some computations   A: x10\n\n✔ 2 of 3 resampling: camels_random_forest (2.1s)\ni   No tuning parameters. `fit_resamples()` will be attempted\ni 3 of 3 resampling: camels_xgboost\n→ A | warning: !  The following columns have zero variance so scaling cannot be used:\n                 gauge_id_new, gauge_id_unknown, high_prec_timing_new,\n                 high_prec_timing_unknown, low_prec_timing_new, low_prec_timing_unknown,\n                 geol_1st_class_new, geol_1st_class_unknown, dom_land_cover_new, and\n                 dom_land_cover_unknown.\n               ℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n                 before normalizing.\nThere were issues with some computations   A: x4\nThere were issues with some computations   A: x10\n\n✔ 3 of 3 resampling: camels_xgboost (3.8s)\n\n# Visualize resampling results\nautoplot(results)\n\n\n\n\n\n\n\n\nModel Selection Based on the visualized metrics, select a model that you think best performs. Describe the reason for your choice using the metrics.\nI chose Boosted Trees because it has the lowest errors with cross-validation and has the highest R-squared.\nDescribe the model you selected. What is the model type, engine, and mode. Why do you think it is performing well for this problem?\nModel Type: Boosted Trees Engine: xgboost Mode: regression It is performing well because XGBoost is good for non-linear relationships, interactions between predictors, dealing with missing values, and avoiding over fitting.\n\nlibrary(tidymodels)\n\n# Tunable XGBoost model specification\nxgb_tune_spec &lt;- boost_tree(\n  trees = 500,                        # fixed number of trees\n  mtry = tune(),                      # tunable: number of variables randomly selected\n  learn_rate = tune(),               # tunable: step size shrinkage\n  tree_depth = tune()                # tunable: max depth of trees\n) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n\nlibrary(workflows)\n\n# Create a workflow object combining the recipe and the tunable XGBoost model\nxgb_workflow &lt;- workflow() %&gt;%\n  add_model(xgb_tune_spec) %&gt;%\n  add_recipe(camels_recipe)\n\n\n# Load required library\nlibrary(tune)\n\n# Extract tunable parameters and their ranges\ndials &lt;- extract_parameter_set_dials(xgb_workflow)\n\n# View the parameter objects and their ranges\ndials$object\n\n[[1]]\n\n\n# Randomly Selected Predictors (quantitative)\n\n\nRange: [1, ?]\n\n\n\n[[2]]\n\n\nTree Depth (quantitative)\n\n\nRange: [1, 15]\n\n\n\n[[3]]\n\n\nLearning Rate (quantitative)\n\n\nTransformer: log-10 [1e-100, Inf]\n\n\nRange (transformed scale): [-3, -0.5]\n\n\n\n# Load required libraries\nlibrary(tidymodels)\nlibrary(dials)\n\ntune_spec &lt;- rand_forest(\n  mtry = tune(),         # Number of predictors to sample\n  min_n = tune(),        # Minimum number of observations in a node\n  trees = 500            # Fixed number of trees\n) %&gt;%\n  set_engine(\"ranger\") %&gt;%\n  set_mode(\"regression\")\n\nwf_tune &lt;- workflow() %&gt;%\n  add_recipe(camels_recipe) %&gt;%\n  add_model(tune_spec)\n\n# Ensure the k-fold cross-validation folds are defined (e.g., camels_folds)\ncamels_folds &lt;- vfold_cv(camels_train_cleaned, v = 5)  # Example with 5-fold cross-validation\n\n# Extract tunable parameters from the workflow\ndials &lt;- extract_parameter_set_dials(wf_tune)\n\n# Optional: If you're using mtry (which depends on number of predictors), finalize it:\ndials &lt;- finalize(dials, camels_train_cleaned)\n\n# Now create the grid\nmy.grid &lt;- grid_space_filling(\n  dials,   # No need for $parameters\n  size = 25\n)\n\n# Tune the model using grid search\nmodel_params &lt;- tune_grid(\n  wf_tune,\n  resamples = camels_folds,  # Ensure camels_folds is defined as your k-folds\n  grid = my.grid,  # Grid of hyperparameters\n  metrics = metric_set(rmse, rsq, mae),  # Define evaluation metrics\n  control = control_grid(save_pred = TRUE)  # Save predictions\n)\n\n→ A | warning: !  The following columns have zero variance so scaling cannot be used:\n                 gauge_id_new, gauge_id_unknown, high_prec_timing_new,\n                 high_prec_timing_unknown, low_prec_timing_new, low_prec_timing_unknown,\n                 geol_1st_class_new, geol_1st_class_unknown, dom_land_cover_new, and\n                 dom_land_cover_unknown.\n               ℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n                 before normalizing.\n\n\nThere were issues with some computations   A: x1\n\n\nThere were issues with some computations   A: x2\n\n\nThere were issues with some computations   A: x3\n\n\nThere were issues with some computations   A: x4\n\n\nThere were issues with some computations   A: x5\nThere were issues with some computations   A: x5\n\n\n\n\n# Visualize the tuning results\nautoplot(model_params)\n\n\n\n\n\n\n\n\n\ncollect_metrics(model_params)\n\n# A tibble: 75 × 8\n    mtry min_n .metric .estimator   mean     n std_err .config              \n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1     1    24 mae     standard   0.165      5 0.00943 Preprocessor1_Model01\n 2     1    24 rmse    standard   0.217      5 0.0128  Preprocessor1_Model01\n 3     1    24 rsq     standard   0.719      5 0.0241  Preprocessor1_Model01\n 4     3    13 mae     standard   0.124      5 0.00863 Preprocessor1_Model02\n 5     3    13 rmse    standard   0.170      5 0.0126  Preprocessor1_Model02\n 6     3    13 rsq     standard   0.757      5 0.0343  Preprocessor1_Model02\n 7     5    33 mae     standard   0.100      5 0.00685 Preprocessor1_Model03\n 8     5    33 rmse    standard   0.141      5 0.0111  Preprocessor1_Model03\n 9     5    33 rsq     standard   0.787      5 0.0284  Preprocessor1_Model03\n10     8     5 mae     standard   0.0792     5 0.00557 Preprocessor1_Model04\n# ℹ 65 more rows\n\n\n\nlibrary(dplyr)\n\n# View metrics ordered by lowest MAE (Mean Absolute Error)\ncollect_metrics(model_params) %&gt;%\n  filter(.metric == \"mae\") %&gt;%\n  arrange(mean)\n\n# A tibble: 25 × 8\n    mtry min_n .metric .estimator   mean     n std_err .config              \n   &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n 1    50     6 mae     standard   0.0482     5 0.00524 Preprocessor1_Model22\n 2    54    14 mae     standard   0.0490     5 0.00532 Preprocessor1_Model24\n 3    38     2 mae     standard   0.0507     5 0.00529 Preprocessor1_Model17\n 4    52    22 mae     standard   0.0509     5 0.00509 Preprocessor1_Model23\n 5    40     9 mae     standard   0.0512     5 0.00527 Preprocessor1_Model18\n 6    57    32 mae     standard   0.0515     5 0.00524 Preprocessor1_Model25\n 7    43    17 mae     standard   0.0523     5 0.00507 Preprocessor1_Model19\n 8    45    28 mae     standard   0.0539     5 0.00534 Preprocessor1_Model20\n 9    29     8 mae     standard   0.0549     5 0.00541 Preprocessor1_Model13\n10    47    38 mae     standard   0.0557     5 0.00535 Preprocessor1_Model21\n# ℹ 15 more rows\n\n\n\nshow_best(model_params, metric = \"mae\")\n\n# A tibble: 5 × 8\n   mtry min_n .metric .estimator   mean     n std_err .config              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;                \n1    50     6 mae     standard   0.0482     5 0.00524 Preprocessor1_Model22\n2    54    14 mae     standard   0.0490     5 0.00532 Preprocessor1_Model24\n3    38     2 mae     standard   0.0507     5 0.00529 Preprocessor1_Model17\n4    52    22 mae     standard   0.0509     5 0.00509 Preprocessor1_Model23\n5    40     9 mae     standard   0.0512     5 0.00527 Preprocessor1_Model18\n\n\n\nhp_best &lt;- select_best(model_params, metric = \"mae\")\n\n\nfinal_wf &lt;- finalize_workflow(\n  wf_tune,\n  hp_best\n)\n\n\nfinal_fit &lt;- last_fit(\n  final_wf,        # your finalized workflow\n  split = camels_split  # original data split from initial_split()\n)\n\n→ A | warning: !  The following columns have zero variance so scaling cannot be used:\n                 gauge_id_new, gauge_id_unknown, high_prec_timing_new,\n                 high_prec_timing_unknown, low_prec_timing_new, low_prec_timing_unknown,\n                 geol_1st_class_new, geol_1st_class_unknown, dom_land_cover_new, and\n                 dom_land_cover_unknown.\n               ℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n                 before normalizing.\n\n\n\nmetrics_test &lt;- collect_metrics(final_fit)\nprint(metrics_test)\n\n# A tibble: 2 × 4\n  .metric .estimator .estimate .config             \n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard      0.0622 Preprocessor1_Model1\n2 rsq     standard      0.921  Preprocessor1_Model1\n\n\n\npredictions_test &lt;- collect_predictions(final_fit)\n\n\nlibrary(ggplot2)\n\n# Create scatter plot with actual vs predicted values\nggplot(predictions_test, aes(x = runoff_ratio, y = .pred)) +\n  geom_point(aes(color = runoff_ratio), alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"blue\") +\n  geom_abline(intercept = 0, slope = 1, color = \"red\") +\n  scale_color_viridis_c() +\n  labs(\n    title = \"Predicted vs Actual Values\",\n    x = \"Actual Values (Runoff Ratio)\",\n    y = \"Predicted Values\"\n  ) +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\ncamels_data_clean &lt;- camels_data %&gt;%\n  filter(!is.na(runoff_ratio))\n\n\n# Fit the finalized workflow to the full cleaned data\nfinal_fit_full &lt;- fit(final_wf, data = camels_data_clean)\n\nWarning: !  The following columns have zero variance so scaling cannot be used:\n  gauge_id_new, gauge_id_unknown, high_prec_timing_new,\n  high_prec_timing_unknown, low_prec_timing_new, low_prec_timing_unknown,\n  geol_1st_class_new, geol_1st_class_unknown, dom_land_cover_new, and\n  dom_land_cover_unknown.\nℹ Consider using ?step_zv (`?recipes::step_zv()`) to remove those columns\n  before normalizing.\n\n\n\nlibrary(broom)\naugmented_preds &lt;- augment(final_fit_full, new_data = camels_data_clean)\n\n\nlibrary(dplyr)\naugmented_preds &lt;- augmented_preds %&gt;%\n  mutate(residual = (.pred - runoff_ratio)^2)  # or .pred - runoff_ratio for raw residuals\n\n\nlibrary(ggplot2)\nlibrary(patchwork)\n\n# Prediction map\npred_map &lt;- ggplot(augmented_preds, aes(x = gauge_lon, y = gauge_lat, color = .pred)) +\n  geom_point(size = 2) +\n  scale_color_viridis_c(option = \"C\") +\n  labs(title = \"Predicted Runoff Ratio\", color = \"Prediction\") +\n  theme_minimal()\n\n# Residual map\nresid_map &lt;- ggplot(augmented_preds, aes(x = gauge_lon, y = gauge_lat, color = residual)) +\n  geom_point(size = 2) +\n  scale_color_viridis_c(option = \"A\") +\n  labs(title = \"Residuals (Squared)\", color = \"Residual\") +\n  theme_minimal()\n\n# Combine with patchwork\npred_map + resid_map"
  },
  {
    "objectID": "Lab6.html",
    "href": "Lab6.html",
    "title": "Lab6",
    "section": "",
    "text": "Install Packages\n\n\nLoad packages\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(ggplot2)\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.3.0 ──\n✔ broom        1.0.7     ✔ rsample      1.2.1\n✔ dials        1.4.0     ✔ tune         1.3.0\n✔ infer        1.0.7     ✔ workflows    1.2.0\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.3.1     ✔ yardstick    1.3.2\n✔ recipes      1.1.1     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n\nlibrary(powerjoin)\nlibrary(glue)\nlibrary(vip)\n\n\nAttaching package: 'vip'\n\nThe following object is masked from 'package:utils':\n\n    vi\n\nlibrary(baguette)\nlibrary(ggthemes)\nlibrary(patchwork)\nlibrary(tidyr)\n\n\n\nData Download\n\nroot  &lt;- 'https://gdex.ucar.edu/dataset/camels/file'\n\n\n\nGetting the documentation PDF\n\ndownload.file('https://gdex.ucar.edu/dataset/camels/file/camels_attributes_v2.0.pdf', \n              'data/camels_attributes_v2.0.pdf')\n\n\n\nGetting Basin Characteristics\n\ntypes &lt;- c(\"clim\", \"geol\", \"soil\", \"topo\", \"vege\", \"hydro\")\n\n# Where the files live online ...\nremote_files  &lt;- glue('{root}/camels_{types}.txt')\n# where we want to download the data ...\nlocal_files   &lt;- glue('data/camels_{types}.txt')\n\nwalk2(remote_files, local_files, download.file, quiet = TRUE)\n\n# Read and merge data\ncamels &lt;- map(local_files, read_delim, show_col_types = FALSE) \n\ncamels &lt;- power_full_join(camels ,by = 'gauge_id')\n\n\n\nQuestion 1\n\n\nWhat does zero_q_freq represent?\n\n\nzero_q_freq represents the frequency of days, with Q = 0 mm/day\n\n# Map of sites colored by mean flow (q_mean)\nggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = q_mean)) +\n  scale_color_gradient(low = \"pink\", high = \"dodgerblue\") +\n  ggthemes::theme_map()\n\n\n\n\n\n\n\n\n\n\nQuestion 2\n\n\nMake 2 maps of the sites, coloring the points by Aridty and p_mean column\n\n# Map 1: Sites colored by Aridity \naridity_map &lt;- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = aridity)) +\n  scale_color_gradient(low = \"lightyellow\", high = \"brown\") +\n  ggthemes::theme_map() +\n  labs(title = \"Map of Sites Colored by Aridity\",\n       x = \"Longitude\",\n       y = \"Latitude\",\n       color = \"Aridity Index\")\n\n# Map 2: Sites colored by Mean Precipitation (p_mean)\np_mean_map &lt;- ggplot(data = camels, aes(x = gauge_lon, y = gauge_lat)) +\n  borders(\"state\", colour = \"gray50\") +\n  geom_point(aes(color = p_mean)) +\n  scale_color_gradient(low = \"dodgerblue\", high = \"darkblue\") +\n  ggthemes::theme_map() +\n  labs(title = \"Map of Sites Colored by Mean Precipitation (p_mean)\",\n       x = \"Longitude\",\n       y = \"Latitude\",\n       color = \"Mean Precipitation\")\n\n# Combine the two maps into a single visualization using patchwork\ncombined_map &lt;- aridity_map + p_mean_map\n\nprint(combined_map)\n\n\n\n\n\n\n\n\n\n\nModel Preparation\n\ncamels |&gt; \n  select(aridity, p_mean, q_mean) |&gt; \n  drop_na() |&gt; \n  cor()\n\n           aridity     p_mean     q_mean\naridity  1.0000000 -0.7550090 -0.5817771\np_mean  -0.7550090  1.0000000  0.8865757\nq_mean  -0.5817771  0.8865757  1.0000000\n\n\n\n\nVisual EDA\n\n# Create a scatter plot of aridity vs rainfall\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  # Add points colored by mean flow\n  geom_point(aes(color = q_mean)) +\n  # Add a linear regression line\n  geom_smooth(method = \"lm\", color = \"red\", linetype = 2) +\n  # Apply the viridis color scale\n  scale_color_viridis_c() +\n  # Add a title, axis labels, and theme (w/ legend on the bottom)\n  theme_linedraw() + \n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  scale_color_viridis_c() +\n  # Apply log transformations to the x and y axes\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\") + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\")\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nggplot(camels, aes(x = aridity, y = p_mean)) +\n  geom_point(aes(color = q_mean)) +\n  geom_smooth(method = \"lm\") +\n  # Apply a log transformation to the color scale\n  scale_color_viridis_c(trans = \"log\") +\n  scale_x_log10() + \n  scale_y_log10() +\n  theme_linedraw() +\n  theme(legend.position = \"bottom\",\n        # Expand the legend width ...\n        legend.key.width = unit(2.5, \"cm\"),\n        legend.key.height = unit(.5, \"cm\")) + \n  labs(title = \"Aridity vs Rainfall vs Runnoff\", \n       x = \"Aridity\", \n       y = \"Rainfall\",\n       color = \"Mean Flow\") \n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nModel Building\n\n# Splitting the Data \nset.seed(123)\n# Bad form to perform simple transformations on the outcome variable within a \n# recipe. So, we'll do it here.\ncamels &lt;- camels |&gt; \n  mutate(logQmean = log(q_mean))\n\n# Generate the split\ncamels_split &lt;- initial_split(camels, prop = 0.8)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\n# Creating Recipe \n# Create a recipe to preprocess the data\nrec &lt;-  recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  # Log transform the predictor variables (aridity and p_mean)\n  step_log(all_predictors()) %&gt;%\n  # Add an interaction term between aridity and p_mean\n  step_interact(terms = ~ aridity:p_mean) |&gt; \n  # Drop any rows with missing values in the pred\n  step_naomit(all_predictors(), all_outcomes())\n\n# Naive base lm approach\n# Prepare the data\nbaked_data &lt;- prep(rec, camels_train) |&gt; \n  bake(new_data = NULL)\n\n# Interaction with lm\n#  Base lm sets interaction terms with the * symbol\nlm_base &lt;- lm(logQmean ~ aridity * p_mean, data = baked_data)\nsummary(lm_base)\n\n\nCall:\nlm(formula = logQmean ~ aridity * p_mean, data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)    -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity        -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean          1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity:p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n\n# Sanity Interaction term from recipe ... these should be equal!!\nsummary(lm(logQmean ~ aridity + p_mean + aridity_x_p_mean, data = baked_data))\n\n\nCall:\nlm(formula = logQmean ~ aridity + p_mean + aridity_x_p_mean, \n    data = baked_data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.91162 -0.21601 -0.00716  0.21230  2.85706 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      -1.77586    0.16365 -10.852  &lt; 2e-16 ***\naridity          -0.88397    0.16145  -5.475 6.75e-08 ***\np_mean            1.48438    0.15511   9.570  &lt; 2e-16 ***\naridity_x_p_mean  0.10484    0.07198   1.457    0.146    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.5696 on 531 degrees of freedom\nMultiple R-squared:  0.7697,    Adjusted R-squared:  0.7684 \nF-statistic: 591.6 on 3 and 531 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nCorrect Version: prep -&gt; bake -&gt; predict\n\ntest_data &lt;-  bake(prep(rec), new_data = camels_test)\ntest_data$lm_pred &lt;- predict(lm_base, newdata = test_data)\n\n\n\nModel Evaluation: Statistical and Visual\n\nmetrics(test_data, truth = logQmean, estimate = lm_pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n\nggplot(test_data, aes(x = logQmean, y = lm_pred, colour = aridity)) +\n  # Apply a gradient color scale\n  scale_color_gradient2(low = \"brown\", mid = \"orange\", high = \"darkgreen\") +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  theme_linedraw() + \n  labs(title = \"Linear Model: Observed vs Predicted\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")\n\n\n\n\n\n\n\n\n\n\nUsing workflow instead\n\n# Define model\nlm_model &lt;- linear_reg() %&gt;%\n  # define the engine\n  set_engine(\"lm\") %&gt;%\n  # define the mode\n  set_mode(\"regression\")\n\n# Instantiate a workflow ...\nlm_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(lm_model) %&gt;%\n  # Fit the model to the training data\n  fit(data = camels_train) \n\n# Extract the model coefficients from the workflow\nsummary(extract_fit_engine(lm_wf))$coefficients\n\n                   Estimate Std. Error    t value     Pr(&gt;|t|)\n(Intercept)      -1.7758557 0.16364755 -10.851710 6.463654e-25\naridity          -0.8839738 0.16144589  -5.475357 6.745512e-08\np_mean            1.4843771 0.15511117   9.569762 4.022500e-20\naridity_x_p_mean  0.1048449 0.07198145   1.456555 1.458304e-01\n\n\n\n# Making predictions\nlm_data &lt;- augment(lm_wf, new_data = camels_test)\ndim(lm_data)\n\n[1] 135  61\n\n\n\n\nModel Evaluation\n\nmetrics(lm_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n\nggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\n\nSwitch it Up\n\nlibrary(baguette)\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\nrf_wf &lt;- workflow() %&gt;%\n  # Add the recipe\n  add_recipe(rec) %&gt;%\n  # Add the model\n  add_model(rf_model) %&gt;%\n  # Fit the model\n  fit(data = camels_train) \n\n\n\nPredictions\n\nrf_data &lt;- augment(rf_wf, new_data = camels_test)\ndim(rf_data)\n\n[1] 135  60\n\n\n\n\nModel Evaluation\n\nmetrics(lm_data, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.583\n2 rsq     standard       0.742\n3 mae     standard       0.390\n\n\n\nggplot(lm_data, aes(x = logQmean, y = .pred, colour = aridity)) +\n  scale_color_viridis_c() +\n  geom_point() +\n  geom_abline() +\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\n\nA workflow approach\n\nwf &lt;- workflow_set(list(rec), list(lm_model, rf_model)) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv) \n\nautoplot(wf)\n\n\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 4 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.563  0.0247    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.771  0.0259    10 recipe       rand…     1\n3 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     2\n4 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     2\n\n\n\n\nQuestion 3\n\n# XGBoost Model\nxgb_model &lt;- boost_tree() %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n# Neural Network Model\nnn_model &lt;- bag_mlp() %&gt;%\n  set_engine(\"nnet\") %&gt;%\n  set_mode(\"regression\")\n\n# Update the workflow to include the XGBoost and Neural Network models\nwf_updated &lt;- workflow_set(\n  list(rec), \n  list(lm_model, rf_model, xgb_model, nn_model)\n) %&gt;%\n  workflow_map('fit_resamples', resamples = camels_cv)\n\n# View the workflow map\nautoplot(wf_updated)\n\n\n\n\n\n\n\n\n\n# Model Evaluation: Statistical Metrics\neval_metrics &lt;- collect_metrics(wf_updated)\n\nprint(eval_metrics)\n\n# A tibble: 8 × 9\n  wflow_id          .config preproc model .metric .estimator  mean     n std_err\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;\n1 recipe_linear_reg Prepro… recipe  line… rmse    standard   0.569    10  0.0260\n2 recipe_linear_reg Prepro… recipe  line… rsq     standard   0.770    10  0.0223\n3 recipe_rand_fore… Prepro… recipe  rand… rmse    standard   0.565    10  0.0247\n4 recipe_rand_fore… Prepro… recipe  rand… rsq     standard   0.770    10  0.0258\n5 recipe_boost_tree Prepro… recipe  boos… rmse    standard   0.600    10  0.0289\n6 recipe_boost_tree Prepro… recipe  boos… rsq     standard   0.745    10  0.0268\n7 recipe_bag_mlp    Prepro… recipe  bag_… rmse    standard   0.547    10  0.0308\n8 recipe_bag_mlp    Prepro… recipe  bag_… rsq     standard   0.787    10  0.0266\n\n\n\n# Rank the results based on R-squared\nrank_results(wf_updated, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 8 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_bag_mlp    Prepro… rmse    0.547  0.0308    10 recipe       bag_…     1\n2 recipe_bag_mlp    Prepro… rsq     0.787  0.0266    10 recipe       bag_…     1\n3 recipe_rand_fore… Prepro… rmse    0.565  0.0247    10 recipe       rand…     2\n4 recipe_rand_fore… Prepro… rsq     0.770  0.0258    10 recipe       rand…     2\n5 recipe_linear_reg Prepro… rmse    0.569  0.0260    10 recipe       line…     3\n6 recipe_linear_reg Prepro… rsq     0.770  0.0223    10 recipe       line…     3\n7 recipe_boost_tree Prepro… rmse    0.600  0.0289    10 recipe       boos…     4\n8 recipe_boost_tree Prepro… rsq     0.745  0.0268    10 recipe       boos…     4\n\n\n\n\nWhich of the 4 models would you move forward with?\n\n\nNeural Network Model\n\n\nBuild your own\n\n\nData Splitting\n\n# Set a seed for reproducibility\nset.seed(123)\n\n# Splitting the data into training (75%) and testing (25%)\ncamels_split &lt;- initial_split(camels, prop = 0.75)\ncamels_train &lt;- training(camels_split)\ncamels_test  &lt;- testing(camels_split)\n\n# Create a 10-fold cross-validation\ncamels_cv &lt;- vfold_cv(camels_train, v = 10)\n\n# Check the data \nglimpse(camels_train)\n\nRows: 503\nColumns: 59\n$ gauge_id             &lt;chr&gt; \"07184000\", \"08164000\", \"02430085\", \"10172800\", \"…\n$ p_mean               &lt;dbl&gt; 3.255383, 2.933001, 4.232671, 1.943691, 3.589425,…\n$ pet_mean             &lt;dbl&gt; 2.741906, 4.743906, 2.947196, 3.446788, 2.253250,…\n$ p_seasonality        &lt;dbl&gt; 0.405783301, 0.095341135, -0.165291650, -0.353639…\n$ frac_snow            &lt;dbl&gt; 0.045357618, 0.001380127, 0.012676724, 0.47283123…\n$ aridity              &lt;dbl&gt; 0.8422683, 1.6174241, 0.6962971, 1.7733215, 0.627…\n$ high_prec_freq       &lt;dbl&gt; 23.25, 23.75, 22.85, 24.70, 14.65, 20.95, 22.30, …\n$ high_prec_dur        &lt;dbl&gt; 1.332378, 1.503165, 1.248634, 1.403409, 1.181452,…\n$ high_prec_timing     &lt;chr&gt; \"jja\", \"mam\", \"djf\", \"mam\", \"jja\", \"jja\", \"jja\", …\n$ low_prec_freq        &lt;dbl&gt; 271.80, 280.05, 264.65, 277.15, 199.35, 254.35, 2…\n$ low_prec_dur         &lt;dbl&gt; 5.592593, 7.292969, 4.816197, 7.061146, 2.982049,…\n$ low_prec_timing      &lt;chr&gt; \"djf\", \"mam\", \"son\", \"jja\", \"jja\", \"son\", \"djf\", …\n$ geol_1st_class       &lt;chr&gt; \"Mixed sedimentary rocks\", \"Unconsolidated sedime…\n$ glim_1st_class_frac  &lt;dbl&gt; 0.5714790, 0.8655454, 1.0000000, 0.3635059, 1.000…\n$ geol_2nd_class       &lt;chr&gt; \"Siliciclastic sedimentary rocks\", \"Siliciclastic…\n$ glim_2nd_class_frac  &lt;dbl&gt; 0.321433977, 0.134454647, 0.000000000, 0.18768450…\n$ carbonate_rocks_frac &lt;dbl&gt; 0.0000000, 0.0000000, 0.0000000, 0.1876845, 0.000…\n$ geol_porostiy        &lt;dbl&gt; 0.1596, 0.2061, 0.2200, 0.1938, 0.2501, 0.1587, 0…\n$ geol_permeability    &lt;dbl&gt; -15.4824, -13.4365, -13.0000, -12.6177, -13.1728,…\n$ soil_depth_pelletier &lt;dbl&gt; 5.9805447, 39.2885953, 1.0000000, 1.5520000, 1.01…\n$ soil_depth_statsgo   &lt;dbl&gt; 1.3486102, 1.5000000, 1.5000000, 0.5773403, 1.500…\n$ soil_porosity        &lt;dbl&gt; 0.4671097, 0.4278973, 0.4313947, 0.4501201, 0.438…\n$ soil_conductivity    &lt;dbl&gt; 0.6178927, 1.2051835, 1.7015119, 1.2954831, 1.649…\n$ max_water_content    &lt;dbl&gt; 0.6326331, 0.6794842, 0.6565674, 0.2482563, 0.631…\n$ sand_frac            &lt;dbl&gt; 17.50598, 45.07884, 46.04384, 35.16330, 42.00856,…\n$ silt_frac            &lt;dbl&gt; 47.39189, 19.83319, 31.71727, 29.61201, 40.38803,…\n$ clay_frac            &lt;dbl&gt; 35.208759, 34.578309, 22.224495, 13.372886, 17.62…\n$ water_frac           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ organic_frac         &lt;dbl&gt; 0.000000, 0.000000, 0.000000, 0.000000, 0.000000,…\n$ other_frac           &lt;dbl&gt; 0.008806217, 0.433653453, 0.000000000, 0.00000000…\n$ gauge_lat            &lt;dbl&gt; 37.28125, 28.95998, 34.46667, 40.49633, 41.76645,…\n$ gauge_lon            &lt;dbl&gt; -95.03267, -96.68637, -88.28361, -112.57440, -78.…\n$ elev_mean            &lt;dbl&gt; 280.99, 73.09, 150.01, 2085.87, 628.71, 21.88, 48…\n$ slope_mean           &lt;dbl&gt; 3.48891, 4.02264, 6.72984, 135.90273, 19.77674, 2…\n$ area_gages2          &lt;dbl&gt; 510.84, 2124.00, 40.56, 11.07, 100.10, 447.58, 22…\n$ area_geospa_fabric   &lt;dbl&gt; 514.82, 2120.71, 40.73, 76.51, 101.16, 435.17, 22…\n$ frac_forest          &lt;dbl&gt; 0.0078, 0.0740, 0.8644, 0.0366, 1.0000, 0.7118, 1…\n$ lai_max              &lt;dbl&gt; 1.7159759, 1.5532298, 4.1218155, 1.1695170, 4.996…\n$ lai_diff             &lt;dbl&gt; 1.3821999, 1.0236174, 3.3999854, 0.8071901, 4.461…\n$ gvf_max              &lt;dbl&gt; 0.6164677, 0.5824469, 0.8212477, 0.4426817, 0.885…\n$ gvf_diff             &lt;dbl&gt; 0.34059165, 0.21378870, 0.40814469, 0.18577568, 0…\n$ dom_land_cover_frac  &lt;dbl&gt; 0.7303335, 0.6249006, 0.5720276, 1.0000000, 1.000…\n$ dom_land_cover       &lt;chr&gt; \"    Croplands\", \"    cropland/natural vegetation…\n$ root_depth_50        &lt;dbl&gt; 0.1638200, 0.1729291, 0.2107647, 0.1200000, 0.190…\n$ root_depth_99        &lt;dbl&gt; 1.500000, 1.543336, 2.108013, 1.500000, 2.000000,…\n$ q_mean               &lt;dbl&gt; 0.97727648, 0.60892487, 1.69908346, 1.36826231, 1…\n$ runoff_ratio         &lt;dbl&gt; 0.30020326, 0.20761157, 0.40142112, 0.70395067, 0…\n$ slope_fdc            &lt;dbl&gt; 0.4202704, 0.4737146, 1.4846057, 1.1747324, 1.840…\n$ baseflow_index       &lt;dbl&gt; 0.13737231, 0.18917345, 0.47717513, 0.72337094, 0…\n$ stream_elas          &lt;dbl&gt; 3.0168198, 3.1757904, 0.8339423, 1.4891734, 1.742…\n$ q5                   &lt;dbl&gt; 0.0000000000, 0.0035708023, 0.1809597297, 0.37571…\n$ q95                  &lt;dbl&gt; 4.8851050, 2.2323274, 5.4891118, 4.8622098, 5.812…\n$ high_q_freq          &lt;dbl&gt; 63.400000, 39.100000, 11.600000, 6.700000, 3.5000…\n$ high_q_dur           &lt;dbl&gt; 4.269360, 3.910000, 1.459119, 4.785714, 1.521739,…\n$ low_q_freq           &lt;dbl&gt; 254.90000, 234.00000, 77.75000, 0.00000, 62.45000…\n$ low_q_dur            &lt;dbl&gt; 19.992157, 26.590909, 12.341270, 0.000000, 11.354…\n$ zero_q_freq          &lt;dbl&gt; 0.08939083, 0.00109514, 0.00000000, 0.00000000, 0…\n$ hfd_mean             &lt;dbl&gt; 206.1500, 182.8500, 150.8000, 235.0000, 155.6000,…\n$ logQmean             &lt;dbl&gt; -0.02298567, -0.49606039, 0.53008896, 0.31354155,…\n\n\n\nglimpse(camels_test)\n\nRows: 168\nColumns: 59\n$ gauge_id             &lt;chr&gt; \"01013500\", \"01022500\", \"01030500\", \"01057000\", \"…\n$ p_mean               &lt;dbl&gt; 3.126679, 3.608126, 3.274405, 3.570500, 3.467413,…\n$ pet_mean             &lt;dbl&gt; 1.971555, 2.119256, 2.043594, 2.132744, 2.091698,…\n$ p_seasonality        &lt;dbl&gt; 0.18794026, -0.11452959, 0.04735819, 0.07913685, …\n$ frac_snow            &lt;dbl&gt; 0.31344036, 0.24525901, 0.27701840, 0.25115767, 0…\n$ aridity              &lt;dbl&gt; 0.6305587, 0.5873564, 0.6241114, 0.5973236, 0.603…\n$ high_prec_freq       &lt;dbl&gt; 12.95, 20.55, 17.15, 20.35, 15.85, 19.45, 19.60, …\n$ high_prec_dur        &lt;dbl&gt; 1.348958, 1.205279, 1.207746, 1.169540, 1.152727,…\n$ high_prec_timing     &lt;chr&gt; \"son\", \"son\", \"son\", \"son\", \"jja\", \"jja\", \"son\", …\n$ low_prec_freq        &lt;dbl&gt; 202.20, 233.65, 215.60, 239.30, 201.10, 225.55, 2…\n$ low_prec_dur         &lt;dbl&gt; 3.427119, 3.662226, 3.514262, 3.747847, 2.842403,…\n$ low_prec_timing      &lt;chr&gt; \"mam\", \"jja\", \"djf\", \"djf\", \"mam\", \"mam\", \"son\", …\n$ geol_1st_class       &lt;chr&gt; \"Siliciclastic sedimentary rocks\", \"Acid plutonic…\n$ glim_1st_class_frac  &lt;dbl&gt; 0.8159044, 0.5906582, 0.5733054, 0.4511106, 0.622…\n$ geol_2nd_class       &lt;chr&gt; \"Basic volcanic rocks\", \"Siliciclastic sedimentar…\n$ glim_2nd_class_frac  &lt;dbl&gt; 0.1797294524, 0.1646182103, 0.2870100056, 0.40924…\n$ carbonate_rocks_frac &lt;dbl&gt; 0.00000000, 0.00000000, 0.05214009, 0.07743342, 0…\n$ geol_porostiy        &lt;dbl&gt; 0.1714, 0.0710, 0.1178, 0.0251, 0.0109, 0.0600, 0…\n$ geol_permeability    &lt;dbl&gt; -14.7019, -14.2138, -14.4918, -13.9903, -14.1198,…\n$ soil_depth_pelletier &lt;dbl&gt; 7.4047619, 17.4128079, 19.0114144, 3.7591463, 4.0…\n$ soil_depth_statsgo   &lt;dbl&gt; 1.2484079, 1.4918455, 1.4613632, 1.5000000, 1.143…\n$ soil_porosity        &lt;dbl&gt; 0.4611488, 0.4159055, 0.4590910, 0.4041179, 0.439…\n$ soil_conductivity    &lt;dbl&gt; 1.1065225, 2.3750051, 1.2898073, 4.0303923, 1.755…\n$ max_water_content    &lt;dbl&gt; 0.5580548, 0.6262289, 0.6530198, 0.6171002, 0.476…\n$ sand_frac            &lt;dbl&gt; 27.84183, 59.39016, 32.23546, 69.00674, 42.58599,…\n$ silt_frac            &lt;dbl&gt; 55.156940, 28.080937, 51.779182, 22.943292, 43.33…\n$ clay_frac            &lt;dbl&gt; 16.275732, 12.037646, 14.776824, 7.817565, 12.724…\n$ water_frac           &lt;dbl&gt; 5.3766978, 1.2269127, 1.6343449, 0.0000000, 0.000…\n$ organic_frac         &lt;dbl&gt; 0.4087168, 0.0000000, 1.3302776, 0.0000000, 0.000…\n$ other_frac           &lt;dbl&gt; 0.0000000, 0.3584723, 0.0220161, 0.0000000, 1.096…\n$ gauge_lat            &lt;dbl&gt; 47.23739, 44.60797, 45.50097, 44.30399, 44.51172,…\n$ gauge_lon            &lt;dbl&gt; -68.58264, -67.93524, -68.30596, -70.53968, -71.8…\n$ elev_mean            &lt;dbl&gt; 250.31, 92.68, 143.80, 215.59, 450.54, 518.15, 37…\n$ slope_mean           &lt;dbl&gt; 21.64152, 17.79072, 12.79195, 32.68445, 47.54354,…\n$ area_gages2          &lt;dbl&gt; 2252.70, 573.60, 3676.17, 190.92, 195.13, 22.80, …\n$ area_geospa_fabric   &lt;dbl&gt; 2303.95, 620.38, 3676.09, 197.70, 209.50, 24.92, …\n$ frac_forest          &lt;dbl&gt; 0.9063, 0.9232, 0.8782, 0.9415, 0.9963, 1.0000, 0…\n$ lai_max              &lt;dbl&gt; 4.167304, 4.871392, 4.685200, 5.362949, 4.930550,…\n$ lai_diff             &lt;dbl&gt; 3.340732, 3.746692, 3.665543, 4.587077, 4.273607,…\n$ gvf_max              &lt;dbl&gt; 0.8045674, 0.8639358, 0.8585020, 0.9055105, 0.886…\n$ gvf_diff             &lt;dbl&gt; 0.3716482, 0.3377125, 0.3513934, 0.4587420, 0.476…\n$ dom_land_cover_frac  &lt;dbl&gt; 0.8834519, 0.8204934, 0.9752580, 0.5327806, 0.603…\n$ dom_land_cover       &lt;chr&gt; \"    Mixed Forests\", \"    Mixed Forests\", \"    Mi…\n$ root_depth_50        &lt;dbl&gt; NA, 0.2374345, NA, 0.2091862, 0.2134887, 0.190000…\n$ root_depth_99        &lt;dbl&gt; NA, 2.238444, NA, 2.073141, 2.154840, 2.000000, 2…\n$ q_mean               &lt;dbl&gt; 1.6991545, 2.1730621, 1.8201075, 1.8235506, 2.077…\n$ runoff_ratio         &lt;dbl&gt; 0.5434375, 0.6022689, 0.5558590, 0.5107270, 0.599…\n$ slope_fdc            &lt;dbl&gt; 1.5282185, 1.7762798, 1.8711104, 1.5332322, 1.408…\n$ baseflow_index       &lt;dbl&gt; 0.5852260, 0.5544784, 0.5084407, 0.4746775, 0.487…\n$ stream_elas          &lt;dbl&gt; 1.8453242, 1.7027824, 1.3775052, 1.2806809, 1.107…\n$ q5                   &lt;dbl&gt; 0.24110613, 0.20473436, 0.10714920, 0.07816945, 0…\n$ q95                  &lt;dbl&gt; 6.373021, 7.123049, 6.854887, 6.866097, 7.096611,…\n$ high_q_freq          &lt;dbl&gt; 6.10, 3.90, 12.25, 13.85, 9.05, 2.15, 8.25, 5.60,…\n$ high_q_dur           &lt;dbl&gt; 8.714286, 2.294118, 7.205882, 2.429825, 2.154762,…\n$ low_q_freq           &lt;dbl&gt; 41.35, 65.15, 89.25, 82.95, 46.15, 37.25, 65.20, …\n$ low_q_dur            &lt;dbl&gt; 20.170732, 17.144737, 19.402174, 13.825000, 6.888…\n$ zero_q_freq          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ hfd_mean             &lt;dbl&gt; 207.25, 166.25, 184.90, 178.05, 187.55, 190.20, 1…\n$ logQmean             &lt;dbl&gt; 0.530130779, 0.776137301, 0.598895589, 0.60078548…\n\n\n\n\nRecipe\n\n# Load necessary libraries\nlibrary(recipes)\n\n# Define recipe\nrec &lt;- recipe(logQmean ~ aridity + p_mean, data = camels_train) %&gt;%\n  step_normalize(all_predictors()) %&gt;%\n  step_naomit(all_predictors(), all_outcomes())\n\n\n\nDefine 3 Models\n\n# Load necessary libraries\nlibrary(parsnip)\n\n# Define a Random Forest model\nrf_model &lt;- rand_forest() %&gt;%\n  set_engine(\"ranger\", importance = \"impurity\") %&gt;%\n  set_mode(\"regression\")\n\n# Define a Linear Regression model\nlm_model &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;%\n  set_mode(\"regression\")\n\n# Define a Support Vector Machine (SVM) model\nxgb_model &lt;- boost_tree() %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"regression\")\n\n# Check the models\nrf_model\n\nRandom Forest Model Specification (regression)\n\nEngine-Specific Arguments:\n  importance = impurity\n\nComputational engine: ranger \n\n\n\nlm_model\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n\nxgb_model\n\nBoosted Tree Model Specification (regression)\n\nComputational engine: xgboost \n\n\n\n\nWorkflow set ()\n\n# Load necessary libraries\nlibrary(workflows)\nlibrary(tune)\nlibrary(dials)\n\nwf &lt;- workflow_set(\n  preproc = list(rec),\n  models = list(rf_model, lm_model, xgb_model)\n) %&gt;%\n  workflow_map(\"fit_resamples\", resamples = camels_cv)\n\n\n\nEvaluation\n\n\nThe SVM (support vector machine) model is the best because it has the highest r-squared value. r-squared = 0.9125\n\nautoplot(wf)\n\n\n\n\n\n\n\n\n\nrank_results(wf, rank_metric = \"rsq\", select_best = TRUE)\n\n# A tibble: 6 × 9\n  wflow_id          .config .metric  mean std_err     n preprocessor model  rank\n  &lt;chr&gt;             &lt;chr&gt;   &lt;chr&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;int&gt; &lt;chr&gt;        &lt;chr&gt; &lt;int&gt;\n1 recipe_rand_fore… Prepro… rmse    0.549  0.0255    10 recipe       rand…     1\n2 recipe_rand_fore… Prepro… rsq     0.787  0.0179    10 recipe       rand…     1\n3 recipe_boost_tree Prepro… rmse    0.583  0.0242    10 recipe       boos…     2\n4 recipe_boost_tree Prepro… rsq     0.766  0.0173    10 recipe       boos…     2\n5 recipe_linear_reg Prepro… rmse    0.626  0.0273    10 recipe       line…     3\n6 recipe_linear_reg Prepro… rsq     0.725  0.0282    10 recipe       line…     3\n\n\n\n\nExtract and Evaluate\n\n# Select best model based on R-squared\nbest_model_id &lt;- rank_results(wf, rank_metric = \"rsq\") %&gt;% \n  slice_max(order_by = mean, n = 1) %&gt;% \n  pull(wflow_id)\n\nbest_model &lt;- extract_workflow(wf, id = best_model_id)\n\n# Fit final workflow\nfinal_wf &lt;- best_model %&gt;% fit(data = camels_train)\n\n# Make predictions\ntest_results &lt;- augment(final_wf, new_data = camels_test)\n\n# Model evaluation\nmetrics(test_results, truth = logQmean, estimate = .pred)\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.599\n2 rsq     standard       0.724\n3 mae     standard       0.370\n\n\n\n\nVisualization\n\n# Visualization\nggplot(test_results, aes(x = logQmean, y = .pred, color = aridity)) +\n  geom_point() +\n  geom_abline(linetype = 2) +\n  scale_color_viridis_c() +\n  theme_linedraw() +\n  labs(title = \"Observed vs Predicted Streamflow\",\n       x = \"Observed Log Mean Flow\",\n       y = \"Predicted Log Mean Flow\",\n       color = \"Aridity\")"
  }
]